> Another encoding of Unicode is UTF-32, which encodes all Unicode code points in 4 bytes. For things like ASCII, the leading 3 bytes are all 0's. What are some tradeoffs between UTF-32 and UTF-8?

UTF-32's biggest downside is that it uses significantly more memory in many cases. For example, the string "Hello" would be 5 bytes long in UTF-8, but 4*5=20 bytes long in UTF-32, because even ASCII characters requires 4 bytes, even though the first 3 are all 0s and could have been avoided by using UTF-8. In addition, it would break backwards compatability with systems that were built for ASCII only.

However, UTF-32 is more consistent than UTF-8 and can be easier to work with. An array of UTF-32 characters will always have the property that the number of elements equals the number of characters, unlike UTF-8, where characters may or may not need multiple elements. For example, "Joséph" is 6 characters long, so in UTF-32, the array would be 6 elements large, but in UTF-8, it would be 7 elements, because é uses 2 bytes to encode. Complex masks are required to identify the correct number of characters, which 

In summary, it is best to use UTF-8 in situations where you might deal with older devices, and in situations where you are mostly encoding ASCII characters. But if you are using a modern system and encoding Chinese characters, for example, it might be more convenient and less error prone to use UTF-32.

> UTF-8 has a leading 10 on all the bytes past the first for multi-byte code points. This seems wasteful – if the encoding for 3 bytes were instead 1110XXXX XXXXXXXX XXXXXXXX (where X can be any bit), that would fit 20 bits, which is over a million code points worth of space, removing the need for a 4-byte encoding. What are some tradeoffs or reasons the leading 10 might be useful? Can you think of anything that could go wrong with some programs if the encoding didn't include this restriction on multi-byte code points?

The most obvious thing that would go wrong is that older programs designed only for ASCII would experience severe errors. ASCII codes all have a leading 0, so the current system of having a leading 10 means that older programs would simply display a question mark box. Without this, older programs might interpret the non-leading bytes as a different character and display it, which would be even more confusing for the user! Additionally, it is more convenient for writing software to be able to easily identify a non-leading byte, such as a function that counts the number of characters (not bytes) in a string.